# AI Stack (Open WebUI + LiteLLM + Ollama)

## Setup
1. Copy `.env.litellm.example` to `.env.litellm` and fill in your API keys.
2. Run `docker compose up -d`
3. Access:
   - Open WebUI: http://localhost:8080
   - LiteLLM: http://localhost:4000
   - Ollama: http://localhost:11434

## Routing (optional)
You can configure advanced routing in `configs/routing.json`
